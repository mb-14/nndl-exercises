{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - How the backpropagation algorithm works\n",
    "\n",
    "### [Problem #543309](http://neuralnetworksanddeeplearning.com/chap2.html#problem_543309)\n",
    "\n",
    "**Part 1:**\n",
    "The error in the output layer is given by the equation:\n",
    "$$\n",
    "  \\delta^L = \\nabla_a C \\odot \\sigma'(z^L). \\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\nabla_a C$ is the vector of individual partial derivatives $\\partial C / \\partial a^L_j$ for every node $j$ in the output layer and $\\sigma'(z^L)$ is the vector where each element represents the rate of change of the activation function $a^L_j$ with respect to $z^L_j$.\n",
    "\n",
    "Consider the following matrix multiplication:\n",
    "$$\n",
    "  \\Sigma'(z^L) \\nabla_a C \\tag{2}\n",
    "$$\n",
    "The component wise form of the result of this multiplication is:\n",
    "$$\n",
    "  \\begin{bmatrix} \\sigma'(z^L_1) \\cdot \\partial C / \\partial a^L_1 \\\\ \\sigma'(z^L_2) \\cdot \\partial C / \\partial a^L_2 \\\\ \\sigma'(z^L_3) \\cdot \\partial C / \\partial a^L_3 \\\\ . \\\\ . \\\\ . \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is essentially the element wise product of $\\nabla_a C$ and $\\sigma'(z^L)$ which makes (2) the same as (1)\n",
    "\n",
    "**Part 2:**\n",
    "An equation for the error $\\delta^l$ w.r.t to the error in the next layer $l+1$ is given by:\n",
    "$$\n",
    "  \\delta^l = ((w^{l+1})^T  \\delta^{l+1}) \\odot \\sigma'(z^l) \\tag{3}\n",
    "$$\n",
    "For sake of simplicity consider the result of the matrix multiplication $(w^{l+1})^T  \\delta^{l+1}$ to be $A$ whose component form is given by $<a_1, a_2, a_3....>$\\\n",
    "\n",
    "Now consider the following operation:\n",
    "$$\n",
    "    \\Sigma'(z^l) \\cdot A  \\tag{4}\n",
    "$$\n",
    "The component wise form of the result of this operation is:\n",
    "$$\n",
    "  \\begin{bmatrix} \\sigma'(z^L_1) \\cdot a_1 \\\\ \\sigma'(z^L_2) \\cdot a_2 \\\\ \\sigma'(z^L_3) \\cdot a_3 \\\\ . \\\\ . \\\\ . \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is the same as the element wise product of $\\sigma'(z^l)$ and $A$, which makes (4) the same as (3).\n",
    "\n",
    "**Part 3:**\n",
    "From the two parts above we have the following equations:\n",
    "$$\n",
    "    \\delta^L = \\Sigma'(z^L) \\nabla_a C \\tag{1}\n",
    "$$\n",
    "$$\n",
    "    \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\delta^{l+1} \\tag{2}\n",
    "$$\n",
    "\n",
    "If we expand the term $\\delta^{l+1}$ in (2) we get:\n",
    "$$\n",
    "    \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\Sigma'(z^{l+1}) (w^{l+2})^T \\delta^{l+2}\n",
    "$$\n",
    "$\\delta^{l+2}$ can be further expanded into error term of the next layer $l+3$. We can keep expanding the error terms, till we get error term from the final output layer $\\delta^L$. This error term can be expanded from (1), to get:\n",
    "$$ \n",
    " \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\ldots \\Sigma'(z^{L-1}) (w^L)^T \\Sigma'(z^L) \\nabla_a C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Exercise 522523](http://neuralnetworksanddeeplearning.com/chap2.html#exercise_522523)\n",
    "\n",
    "Prove the following equations:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j \\tag{1}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\tag{2}\n",
    "$$\n",
    "\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "Using the chain rule, the left hand side of equation (1) can be written as:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j} \\\\\n",
    "\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial (\\sum_{i} w^l_{ji}\\cdot a^{l-1}_i + b^l_j)}{\\partial b^l_j} \\\\\n",
    "\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial z^l_j}$ is just $\\delta^l_j$, hence (1) is true\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "Using the chain rule, the left hand side of equation (1) can be written as:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}} \\\\\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial (\\sum_{i} w^l_{ji}\\cdot a^{l-1}_i + b^l_j)}{\\partial w^l_{jk}} \\\\\n",
    "$$\n",
    "\n",
    "The weight $w^l_{ji}$ depends on the $w^l_{jk}$ only when $k = i$, hence the equation above can be reduced to:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial w^l_{jk}\\cdot a^{l-1}_k}{\\partial w^l_{jk}} \\\\\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j} a^{l-1}_k \\\\\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial z^l_j}$ is just $\\delta^l_j$, hence (2) is true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Exercise 675621](http://neuralnetworksanddeeplearning.com/chap2.html#exercises_675621)\n",
    "\n",
    "- **Backpropagation with a single modified neuron:** Suppose we modify a single neuron in a feedforward network so that the output from the neuron is given by f(∑jwjxj+b), where f is some function other than the sigmoid. How should we modify the backpropagation algorithm in this case?\n",
    "- **Backpropagation with linear neurons:** Suppose we replace the usual non-linear σ\n",
    " function with σ(z)=z throughout the network. Rewrite the backpropagation algorithm for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "Since the BP algorithm works with partial derivatives computed with respect to the weighted outputs (z) and not the activation outputs (a), the steps of the algorithm would remain the same. We would just have to change how we compute the error ($\\delta$) for the modified neuron. More specifically, whenever we would replace $\\sigma'(z)$ with $f'(z)$ in the BP equations for the modified neuron.\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "If we replace the non-linear activation function $\\sigma$ with $\\sigma(z) = z$, then the BP algorithm is simplified. The error in the output layer simply becomes:\n",
    "$$\n",
    "    \\delta^L = \\nabla_a C\n",
    "$$\n",
    "For the hidden layers, the error becomes:\n",
    "$$\n",
    "\\delta^{l} = (w^{l+1})^T \\delta^{l+1}\n",
    "$$\n",
    "\n",
    "For each layer, the computed output becomes a linear function of $\\nabla_C$. This potentially allows us to calculate the errors for the each layer and subsequently the layer parameters in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Problem 269962](http://neuralnetworksanddeeplearning.com/chap2.html#problem_269962)\n",
    "\n",
    "Fully matrix-based approach to backpropagation over a mini-batch Our implementation of stochastic gradient descent loops over training examples in a mini-batch. It's possible to modify the backpropagation algorithm so that it computes the gradients for all training examples in a mini-batch simultaneously. The idea is that instead of beginning with a single input vector, x, we can begin with a matrix $X = [x_1 x_2 \\ldots x_m]$ whose columns are the vectors in the mini-batch. We forward-propagate by multiplying by the weight matrices, adding a suitable matrix for the bias terms, and applying the sigmoid function everywhere. We backpropagate along similar lines. Explicitly write out pseudocode for this approach to the backpropagation algorithm. Modify network.py so that it uses this fully matrix-based approach. The advantage of this approach is that it takes full advantage of modern libraries for linear algebra. As a result it can be quite a bit faster than looping over the mini-batch. (On my laptop, for example, the speedup is about a factor of two when run on MNIST classification problems like those we considered in the last chapter.) In practice, all serious libraries for backpropagation use this fully matrix-based approach or some variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 5755 / 10000\n",
      "Epoch 1: 5890 / 10000\n",
      "Epoch 2: 6735 / 10000\n",
      "Epoch 3: 7294 / 10000\n",
      "Epoch 4: 7615 / 10000\n",
      "Epoch 5: 7642 / 10000\n",
      "Epoch 6: 7696 / 10000\n",
      "Epoch 7: 7689 / 10000\n",
      "Epoch 8: 7737 / 10000\n",
      "Epoch 9: 7755 / 10000\n",
      "Epoch 10: 7808 / 10000\n",
      "Epoch 11: 8602 / 10000\n",
      "Epoch 12: 8646 / 10000\n",
      "Epoch 13: 8651 / 10000\n",
      "Epoch 14: 8633 / 10000\n",
      "Epoch 15: 8661 / 10000\n",
      "Epoch 16: 8674 / 10000\n",
      "Epoch 17: 8654 / 10000\n",
      "Epoch 18: 8679 / 10000\n",
      "Epoch 19: 8672 / 10000\n",
      "Epoch 20: 8688 / 10000\n",
      "Epoch 21: 8670 / 10000\n",
      "Epoch 22: 8689 / 10000\n",
      "Epoch 23: 8698 / 10000\n",
      "Epoch 24: 8667 / 10000\n",
      "Epoch 25: 8687 / 10000\n",
      "Epoch 26: 8689 / 10000\n",
      "Epoch 27: 8683 / 10000\n",
      "Epoch 28: 8694 / 10000\n",
      "Epoch 29: 8710 / 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We update two functions in the original Network class in network.py:\n",
    "1. update_mini_batch: We column stack the inputs and the outputs in\n",
    "        the mini batch and pass them to the backprop method. The gradient\n",
    "        errors in nabla_b and nabla_w are then used to update the weights \n",
    "        and biases in each layer.\n",
    "\n",
    "2. backprop: We don't need to change much besides summing the \n",
    "        biases for all examples in the mini batch. \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from scripts import mnist_loader, network\n",
    "\n",
    "\n",
    "class VectorNetwork(network.Network):\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        X, Y = zip(*mini_batch)\n",
    "        X = np.column_stack(X)\n",
    "        Y = np.column_stack(Y)\n",
    "        batch_size = len(mini_batch)\n",
    "        nabla_b, nabla_w = self.backprop(X, Y)\n",
    "        self.weights = [w - (eta / batch_size) * nw for w,\n",
    "                        nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / batch_size) * nb for b,\n",
    "                       nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = X\n",
    "        activations = [X]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = network.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        cd = self.cost_derivative(activations[-1], Y)\n",
    "        sp = network.sigmoid_prime(zs[-1])\n",
    "        delta = cd * sp\n",
    "        nabla_b[-1] = delta.sum(1).reshape([len(delta), 1])\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = network.sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta.sum(1).reshape([len(delta), 1])\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "net = VectorNetwork([784, 100, 10])\n",
    "# net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, epochs=30, mini_batch_size=10,\n",
    "        eta=3.0, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
