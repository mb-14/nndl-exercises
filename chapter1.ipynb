{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Using neural nets to recognize handwritten digits\n",
    "\n",
    "### [Exercise #191892](http://neuralnetworksanddeeplearning.com/chap1.html#exercises_191892)\n",
    "\n",
    "**Sigmoid neurons simulating perceptrons, part I**\n",
    "\n",
    "Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c>0. Show that the behaviour of the network doesn't change.\n",
    "\n",
    "\n",
    "**Sigmoid neurons simulating perceptrons, part II**\n",
    "\n",
    "Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w⋅x+b≠0$ for the input x to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant $c>0$. Show that in the limit as $c→∞$\n",
    "the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w⋅x+b=0$\n",
    "for one of the perceptrons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "The output for a perceptron is given by\n",
    "$$\n",
    "output = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & if\\ w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & if\\ w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "  \\tag{1}\n",
    "$$\n",
    "\n",
    "where w and b are the weights and biases respectively. If we multiply the weights and biases with a positive constant c, the output before applying the activation function changes to\n",
    "$$\n",
    "c\\cdot w \\cdot x + c\\cdot b \\tag{2}\n",
    "$$\n",
    "The c can be factored out from (2) and written as:\n",
    "$$\n",
    "c\\cdot (w\\cdot x + b)  \n",
    "$$\n",
    "Since the sign of (2) does not change when multiplied with a positive constant, the output of (1) will not change and the decision boundary remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "If all the neurons in the perceptron network are replace by a sigmoid neuron then the output becomes:\n",
    "$$\n",
    "output = \\frac{1}{1 + e^{-(w\\cdot x+b)}}\n",
    "$$\n",
    "If you multiply the weights and biases with a positive constant c, then the output becomes:\n",
    "$$\n",
    "output = \\frac{1}{1 + e^{-c\\cdot (w\\cdot x+b)}}\n",
    "$$\n",
    "Now consider the behavior of the function as $c \\to \\infty$ in the following cases:\n",
    "\n",
    "Case 1: $w\\cdot x + b > 0$\\\n",
    "In this scenario, as $c \\to \\infty$ the output approaches 1.\n",
    "\n",
    "Case 2: $w\\cdot x + b < 0$\\\n",
    "In this scenario, as $c \\to \\infty$ the output approaches 0.\n",
    "\n",
    "This mimics the behavior of the piecewise output function (1) given in part 1 and hence the network mimics the output of a perceptron network when $c \\to \\infty$\n",
    "\n",
    "When $w\\cdot x + b = 0$ then irrespective of the value of c, the output is always 1/2 which is inconsistent with the outputs of the perceptron network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### http://neuralnetworksanddeeplearning.com/chap1.html#exercise_513527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.04, -1.0, -1.0, -0.98) [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "w_0 = np.full(10, -1)\n",
    "w_0[[1, 3, 5, 7, 9]] = 1\n",
    "w_1 = np.full(10, -1)\n",
    "w_1[[2, 3, 6, 7]] = 1\n",
    "w_2 = np.full(10, -1)\n",
    "w_2[[4, 5, 6, 7]] = 1\n",
    "w_3 = np.full(10, -1)\n",
    "w_3[[8, 9]] = 1\n",
    "\n",
    "\n",
    "## Bias can be zero since we are using the sigmoid activation\n",
    "def new_repr(output):\n",
    "    a_0 = np.sum(w_0 * output)\n",
    "    a_1 = np.sum(w_1 * output)\n",
    "    a_2 = np.sum(w_2 * output)\n",
    "    a_3 = np.sum(w_3 * output)\n",
    "\n",
    "    return a_3, a_2, a_1, a_0\n",
    "\n",
    "def new_binary_repr(new_output):\n",
    "    sigmoid_op = np.apply_along_axis(sigmoid, 0, new_output)\n",
    "    return (sigmoid_op > 0.5).astype(int)\n",
    "\n",
    "output = np.full(10, 0.01)\n",
    "\n",
    "## Change index to desired output\n",
    "output[0] = 0.99\n",
    "\n",
    "new_output = new_repr(output)\n",
    "new_binary_output = new_binary_repr(new_output)\n",
    "print(new_output, new_binary_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### http://neuralnetworksanddeeplearning.com/chap1.html#exercises_647181\n",
    "\n",
    "- For a cost function C which depends on a set of variables $v = (v_1, v_2, v_3...v_n)^T$, prove that the choice of $\\Delta v$\n",
    "given the constraint $\\left \\| \\Delta v \\right \\| = \\epsilon$ which decreases C as much as possible is $\\eta \\nabla C$ where $\\eta = \\frac{\\epsilon}{{\\left \\| \\nabla C \\right \\|}}$ \n",
    "\n",
    "- I explained gradient descent when C is a function of two variables, and when it's a function of more than two variables. What happens when C is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "According to the [Cauchy-Shwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), we can say that:\n",
    "$$\n",
    "    \\Delta v \\cdot \\nabla C \\leq \\left \\| \\Delta v \\right \\| \\left \\| \\nabla C \\right \\|\n",
    "$$\n",
    "\n",
    "The left hand side represents the change in C given change in the variables v and the maximum value of this change that is possible is $\\left \\| \\Delta v \\right \\| \\left \\| \\nabla C \\right \\|$\n",
    "\n",
    "If we set the value of $\\Delta v$ to $\\eta \\nabla C$, the left hand side becomes:\n",
    "$$\n",
    "    \\eta \\left \\| \\nabla C \\right \\|^2\n",
    "$$\n",
    "\n",
    "Substituting the value of $\\eta$ with $\\frac{\\epsilon}{{\\left \\| \\nabla C \\right \\|}}$ you get\n",
    "$$\n",
    "\\epsilon \\left \\| \\nabla C \\right \\|\n",
    "$$\n",
    "\n",
    "$\\epsilon$ can be further substituted with $\\left \\| \\Delta v \\right \\|$ to get $\\left \\| \\Delta v \\right \\| \\left \\| \\nabla C \\right \\|$ which is same as the maximum possible change to C.\n",
    "\n",
    "This proves that setting $\\Delta v = \\eta \\nabla C$ gives us the optimal change for C in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "If C is a function of just one variable, then gradient descent moves in the direction of the tangent of the function at a point towards the function minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### http://neuralnetworksanddeeplearning.com/chap1.html#exercise_263792\n",
    "\n",
    "An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, x, we update our weights and biases according to the rules wk→w′k=wk−η∂Cx/∂wk and bl→b′l=bl−η∂Cx/∂bl. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A advantage of online learning is that the computation time for each iteration is very less since we update the network parameters based on a single example\n",
    "\n",
    "A disadvantage of online learning is that the path taken by gradient descent algorithm to reach the minima is going to be more noisier and longer compared to SGD with a mini-batch of 20 since the gradient computed for a mini-batch in each iteration is a better estimate of the true gradient compared to the gradient computed for a single example.\n",
    "\n",
    "**Path taken by mini-batch SGD:**\n",
    "\n",
    "![minibatch](./resources/minibatch_sgd.png)\n",
    "\n",
    "**Path taken by online SGD:**\n",
    "\n",
    "![online](./resources/online_sgd.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### [Exercise #717502](http://neuralnetworksanddeeplearning.com/chap1.html#exercise_717502)\n",
    "\n",
    "Write out Equation $a' = \\sigma(wa+b)$ in component form, and verify that it gives the same result as the rule (4) for computing the output of a sigmoid neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the weights and biases in a neural network between two consecutive layers i and i+1. Let's say layer i has m nodes and layer i+1 has n nodes. This means that w would have the shape (n,m), a would be a column vector with length m and b would be a column vector with length n.\n",
    "\n",
    "The equation above can be written as:\n",
    "$$\n",
    "a' = \\sigma(w_{n\\times m}\\times a_m + b_n)\n",
    "$$\n",
    "In component form, we multiply the matrices w and a and add b to the result to get:\n",
    "$$\n",
    "a' = \\sigma(<\\sum_{j=1}^{m} w_{1,j}\\cdot a_j +b_1,\\\\\\sum_{j=1}^{m} w_{2,j}\\cdot a_j +b_2,\\\\.\\\\.\\\\.\\\\\\sum_{j=1}^{m} w_{n,j}\\cdot a_j +b_n>)\n",
    "$$\n",
    "We then apply the sigmoid function to each element. The resulting array represents the output for all the nodes in (i+1)_{th} layer, we each node has the following output:\n",
    "$$\n",
    "    output = \\frac{1}{1+e^{-\\sum_{j} w_j\\cdot a_j +b}}\n",
    "$$\n",
    "\n",
    "This is the same as equation (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### [Exercise #420023](http://neuralnetworksanddeeplearning.com/chap1.html#exercise_420023)\n",
    "\n",
    "Try creating a network with just two layers - an input and an output layer, no hidden layer - with 784 and 10 neurons, respectively. Train the network using stochastic gradient descent. What classification accuracy can you achieve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 4554 / 10000\n",
      "Epoch 1: 4782 / 10000\n",
      "Epoch 2: 5536 / 10000\n",
      "Epoch 3: 6602 / 10000\n",
      "Epoch 4: 6631 / 10000\n",
      "Epoch 5: 6674 / 10000\n",
      "Epoch 6: 6668 / 10000\n",
      "Epoch 7: 6654 / 10000\n",
      "Epoch 8: 6672 / 10000\n",
      "Epoch 9: 6659 / 10000\n",
      "Epoch 10: 6679 / 10000\n",
      "Epoch 11: 6691 / 10000\n",
      "Epoch 12: 6683 / 10000\n",
      "Epoch 13: 6687 / 10000\n",
      "Epoch 14: 6676 / 10000\n",
      "Epoch 15: 6690 / 10000\n",
      "Epoch 16: 6685 / 10000\n",
      "Epoch 17: 6696 / 10000\n",
      "Epoch 18: 6691 / 10000\n",
      "Epoch 19: 6679 / 10000\n",
      "Epoch 20: 6687 / 10000\n",
      "Epoch 21: 6691 / 10000\n",
      "Epoch 22: 6689 / 10000\n",
      "Epoch 23: 6715 / 10000\n",
      "Epoch 24: 6690 / 10000\n",
      "Epoch 25: 6700 / 10000\n",
      "Epoch 26: 6701 / 10000\n",
      "Epoch 27: 6710 / 10000\n",
      "Epoch 28: 6712 / 10000\n",
      "Epoch 29: 6705 / 10000\n"
     ]
    }
   ],
   "source": [
    "from scripts import mnist_loader, network\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "net = network.Network([784, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data = test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
